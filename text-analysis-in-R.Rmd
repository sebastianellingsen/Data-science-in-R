---
title: Text analysis using R
#subtitle: Econometrics lll
author: Sebastian Ellingsen
institute: Department of Economics
titlegraphic: /Dropbox/teaching/clemson-academic.png
fontsize: 10pt
output:
 ioslides_presentation:
    smaller: true
    #logo: ~/Dropbox/teaching/clemson-paw-transparent.png
    #css: ~/Users/sebastianellingsen/Dropbox/markdown_templates/svm-r-markdown-templates-master/svm-ioslides-css.css    
 beamer_presentation:
    template: /Users/sebastianellingsen/Desktop/Datascience/svm-r-markdown-templates-master/svm-latex-beamer.tex
    keep_tex: true
    slide_level: 2
---



```{r, echo=FALSE, message=FALSE, warning=FALSE}
require("knitr")
opts_chunk$set(message=FALSE, warning=FALSE)
```



## Introduction
Text can be stored in a dataframe in R, but it's convinient to use `tidytext` commands for several tasks in analyzing text with R.

Especially for descriptive purposes and data exploration, these tools are very useful and easy to use.

These slides will show the most important operations needed to do basic text analysis in R. A key reference and many of the examples come from Silge and Robinson (2017).

First, let's run through a few introductory examples.

# Loading and cleaning data

## Simple example

Concider the poem by @Limericking (Twitter),
```{r, echo=TRUE}
text <- c("A painting by Banksy was smart;",
"At auction, it shredded apart.",
"Now tattered, in pieces,",
"Its value increases,",
"For such is the market for art.")


```

This can be stored as a dataframe:
```{r, echo=TRUE}
library(dplyr)
text_df <- data_frame(line = 1:5, text = text)

text_df
```
 

## Simple example (cont.)

We wwant it as a one-token-per-document-per-row format. This is achieved with the unnest_tokens command.

```{r, echo=TRUE}
library(tidytext)

tidy_text <- text_df %>%
  unnest_tokens(word, text)

tidy_text
```



## Simple example (cont.)

Many tools in the tidytext package can be used directly on the textfile when it is stored in the tidy format. Especially for descriptive purposes it's very useful.

However, there are many words that are not very informative about the content. These are stop words and we will see how to remove them in the following example. 


## Another example

Tidytext has several buil in books to work with. Let's have a look at the Jane Austen novels. The code below loads the data and unnests the data as a tidy dataset. 


```{r, echo=TRUE, results='hide'}

library(janeaustenr)
library(dplyr)
library(stringr)

original_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%
  ungroup()

# Unnesting the data
tidy_books <- original_books %>%
  unnest_tokens(word, text)

tidy_books


```


## Example continued

Note that the most common words in the books are stop words. We can easily remove these by using anti_join from the tidyverse package. `tidy_books` now gives a tidy dataset of the books. The next step is to remove stop words:

```{r, echo=TRUE, results='hide'}
data(stop_words)

tidy_books <- tidy_books %>%
  anti_join(stop_words)

```


The `stop_words` dataset in the tidytext package contains stop words from three lexicons. 


## Another example (cont.)
Plotting the most used words after removing stop words:
```{r, echo=FALSE}
library(ggplot2)

tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

```


## Last example

Let’s get The Time Machine, The War of the Worlds, The Invisible Man, and The Island of Doctor Moreau from the Gutenberg project.

```{r, echo=TRUE, results='hide'}

library(gutenbergr)

hgwells <- gutenberg_download(c(35, 36, 5230, 159))

```

Then we unnest the data as before:
```{r, echo=TRUE, results='hide'}

tidy_hgwells <- hgwells %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

```


## Last example (cont.)

What are the most common words in the novel?

```{r, echo=FALSE}
tidy_hgwells %>%
  count(word, sort = TRUE)

```

## Last example (cont.)

Loading and tidying the data:
```{r, echo=TRUE}
bronte <- gutenberg_download(c(1260, 768, 969, 9182, 767))
tidy_bronte <- bronte %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

```


Comparing the three books:
```{r, echo=TRUE}
library(tidyr)

frequency <- bind_rows(mutate(tidy_bronte, author = "Brontë Sisters"),
                       mutate(tidy_hgwells, author = "H.G. Wells"), 
                       mutate(tidy_books, author = "Jane Austen")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(author, word) %>%
  group_by(author) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(author, proportion) %>% 
  gather(author, proportion, `Brontë Sisters`:`H.G. Wells`)
```


 
## Plotting word frequencies
```{r, echo=FALSE}
library(scales)

ggplot(frequency, aes(x = proportion, y = `Jane Austen`, color = abs(`Jane Austen` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Jane Austen", x = NULL)
```






# Sentiment analysis


## Sentiment analysis

Often researcher are interested in finding features of text such as the sentiment, but they are constrained in some way.

Here we cover dictionary based sentiment analysis which is the most straight forward way of doing sentiment analysis. 

The idea is to exploit hand coded collections of words that are termed positive/negative and look how often these occur in a given document. 

This can easily be done in a tidyverse setting using dictionaries as well as join operations.



## Dictionaries

```{r, echo=TRUE, results='hide'}

library(tidytext)

sentiments

get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")

```



## Sentiment analysis - Example

```{r, echo=TRUE, results='hide'}
library(janeaustenr)
library(dplyr)
library(stringr)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)


```


## How many words are associated with joy?
```{r, echo=TRUE, results='hide'}

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)

```



## How does the sentiment change throughout the novel?
```{r, echo=TRUE, results='hide'}

library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

```


## Making a wordcloud of words in Sense & Sensibility
```{r, echo=TRUE, results='hide'}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  filter(book=="Sense & Sensibility") %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", 
                                                 ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)


```


## Making a wordcloud
```{r, echo=FALSE, results='hide'}
library(wordcloud)

tidy_books %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))


```


## Removing stop words
```{r, echo=FALSE, results='hide'}
library(wordcloud)

tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))


```





# Analysing term frequency


This code sums up total words and frequency of each word by book and then joins the two.
```{r, echo=TRUE, results='hide'}

library(dplyr)
library(janeaustenr)
library(tidytext)

book_words <- austen_books() %>%
  unnest_tokens(word, text) %>%
  count(book, word, sort = TRUE) %>%
  ungroup()

total_words <- book_words %>% 
  group_by(book) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

book_words

```






# Analysing term frequency


```{r, echo=TRUE, results='hide'}

library(ggplot2)

ggplot(book_words, aes(n/total, fill = book)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0009) +
  facet_wrap(~book, ncol = 2, scales = "free_y") + theme_void()

```





# n-grams




## Bigrams
```{r, echo=TRUE, results='hide'}

library(dplyr)
library(tidytext)
library(janeaustenr)

austen_bigrams <- austen_books() %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2)

austen_bigrams


```



```{r, echo=TRUE, results='hide'}

austen_bigrams %>%
  count(bigram, sort = TRUE)

```



## Remove cases where either is a stopword
```{r, echo=TRUE, results='hide'}

library(tidyr)

bigrams_separated <- austen_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts
```


We can reunite the bigrams after having removed the stopwords:

```{r, echo=TRUE, results='hide'}

bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united
```


## We can analyze word frequency using bigrams:

```{r, echo=TRUE, results='hide'}

bigram_tf_idf <- bigrams_united %>%
  count(book, bigram) %>%
  bind_tf_idf(bigram, book, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf
```


Note: the larger the n-gram the more sparse your data will become. Usually better with large datasets.





# Other text formats

## Loading the `AssociatedPress` package


```{r, echo=TRUE, results='hide'}
library(tm)

data("AssociatedPress", package = "topicmodels")
AssociatedPress
```

We see that this dataset contains 2246 documents (each of them an AP article) and 10473 terms (distinct words). Notice that this DTM is 99% sparse (99% of document-word pairs are zero).


## Can be organized as tidy text using `tidy`

```{r, echo=TRUE, results='hide'}
library(dplyr)
library(tidytext)

ap_td <- tidy(AssociatedPress)
ap_td

```


## Can be organized as tidy text using `tidy`

```{r, echo=TRUE, results='hide'}
library(dplyr)
library(tidytext)

ap_td <- tidy(AssociatedPress)
ap_td

```



# Topic modeling


## Topic modeling

Start by running the latent drichlet model for the associated press dataset.

```{r, echo=TRUE, results='hide'}
library(topicmodels)

data("AssociatedPress")
AssociatedPress

#ap_td <- tidy(AssociatedPress)

ap_td %>%
  cast_dtm(document, term, count)


```


```{r, echo=TRUE, results='hide'}
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_lda
```


## Topic modeling


The results can be tidied in order to use tidy tools.
```{r, echo=TRUE, results='hide'}
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```



# Visualizing the results

The two categories seem to be about politics and the economy.
```{r, echo=TRUE, results='hide'}
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```



# Document topic probabilities


```{r, echo=TRUE, results='hide'}

ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents


```


## Text analysis: supervised classification


# Initialization


```{r, echo=TRUE, results='hide'}

library(tidyverse)
library(tidytext)
library(stringr)
library(caret)
library(tm)

set.seed(1234)
theme_set(theme_minimal())


```


```{r, echo=TRUE, results='hide'}

data(USCongress, package = "RTextTools")

(congress <- as_tibble(USCongress) %>%
    mutate(text = as.character(text)))


```







# Reducing model complexity

Convert to tidy dataframe, then remove numbers, remove stop words, lastly we stem the dataset.
```{r, echo=TRUE, results='hide'}

(congress_tokens <- congress %>%
   unnest_tokens(output = word, input = text) %>%
   # remove numbers
   filter(!str_detect(word, "^[0-9]*$")) %>%
   # remove stop words
   anti_join(stop_words) %>%
   # stem the words
   mutate(word = SnowballC::wordStem(word)))

```

Note: we can plot wordclouds without using the 

We need the data as a document term matrix in order to use learning algorithms.
```{r, echo=TRUE, results='hide'}

(congress_dtm <- congress_tokens %>%
   # get count of each token in each document
   count(ID, word) %>%
   # create a document-term matrix with all features and tf weighting
   cast_dtm(document = ID, term = word, value = n))

congress_dtm <- removeSparseTerms(congress_dtm, sparse = .99)

```




# Exploratory analysis

Tidytext methods are very useful for exploratory analysis. 


```{r, echo=TRUE, results='hide'}

congress_tfidf <- congress_tokens %>%
   count(major, word) %>%
   bind_tf_idf(word, major, n)


```




```{r, echo=TRUE, results='hide'}

plot_congress <- congress_tfidf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))

# graph the top 10 tokens for 4 categories
plot_congress %>%
  filter(major %in% c(1, 2, 3, 6)) %>%
  mutate(major = factor(major, levels = c(1, 2, 3, 6),
                        labels = c("Macroeconomics", "Civil Rights",
                                   "Health", "Education"))) %>%
  group_by(major) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf)) +
  geom_col() +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~major, scales = "free") +
  coord_flip()

```




We are now interested in predicting the category using the most common words.

```{r, echo=TRUE, results='hide'}

#congress_rf <- train(x = as.matrix(congress_dtm),
#                     y = factor(congress$major),
#                     method = "rf",
#                     ntree = 10,
#                     trControl = trainControl(method = "oob"))

#congress_rf$finalModel


  congress_rf_2000 <- train(x = as.matrix(congress_dtm),
                           y = factor(congress$major),
                           method = "rf",
                           ntree = 10,
                           trControl = trainControl(method = "oob"))
```



```{r, echo=TRUE, results='hide'}

library(randomForest)
varImpPlot(congress_rf_2000$finalModel)

```



# notes
A common practice is to keep only the words within each document i with tf-idf scores above some rank or cutoff.


final step that is commonly used to reduce the feature space is stemming:


remove words with lower than ... itdf score



all these tricks reduce the dimensionality of the data and makes it less computationally costly to conduct prediction.


igoring the order of words is sometimes called the bag of words approach


A phrase of length n is referred to as an n-gram
costly to use high n-grams
Best practice in many cases is to begin analysis by focusing on single words. Given the accuracy obtained with words alone, one can then evaluate if it is worth the extra time to move on to 2-grams or 3-grams.

hva er strukturen? 
fjerne ord som dukker opp hele tiden, fa en matrix som er mindre sparse. 
sa lag en wordcloud 


If instead there is a highly
non-linear and complex relationship between the features and the response
as indicated by model (8.9), then decision trees may outperform classical
approaches.

idea: groseclose and gentzkow papers, what politicians do the newspapers sound more like?
predict party affiliation using speech, then compare predictive terms to newspapers

might be highly non-linear, the feature space is very sparse, outcome is binary, 
however, it has the flavor of a supervised learning problem because I observe the 
outcomes in the training set

two methods commonly used in this setting, support vector machine and random forrest classification  

basic algorithm: pre-processing, cross-validation, find predictive words, predict 
                 party affiliation of newspapers





